"""
LangChain RAG Pipeline - Core AI intelligence for book interactions
"""
from typing import List, Dict, Any, Optional
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
import uuid
import structlog

from core.config import settings
from models.chat import ChatMode

logger = structlog.get_logger()


class LangChainPipeline:
    """LangChain RAG pipeline for intelligent book interactions"""
    
    def __init__(self):
        # Initialize Gemini for chat
        self.llm = ChatGoogleGenerativeAI(
            model=settings.GEMINI_MODEL,
            google_api_key=settings.GOOGLE_API_KEY,
            temperature=0.7,
            convert_system_message_to_human=True
        )
        
        # DON'T initialize embedding model here - will be lazy loaded per worker
        self._embedding_model = None
        self.embedding_dimension = 384
        
        # Initialize Qdrant
        self.qdrant = QdrantClient(url=settings.QDRANT_URL)
        
        # Text splitter for chunking
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        logger.info("langchain_pipeline_initialized", embedding_model="all-MiniLM-L6-v2")
    
    @property
    def embedding_model(self):
        """Lazy load embedding model per worker process"""
        if self._embedding_model is None:
            logger.info("loading_embedding_model_in_worker")
            self._embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        return self._embedding_model
    
    async def create_book_collection(self, book_id: str) -> str:
        """Create a Qdrant collection for a book"""
        collection_name = f"book_{book_id}"
        
        try:
            # Check if collection exists
            collections = self.qdrant.get_collections().collections
            exists = any(c.name == collection_name for c in collections)
            
            if not exists:
                self.qdrant.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=self.embedding_dimension,  # 384 for all-MiniLM-L6-v2
                        distance=Distance.COSINE
                    )
                )
                logger.info("qdrant_collection_created", collection=collection_name)
            
            return collection_name
        except Exception as e:
            logger.error("failed_to_create_collection", error=str(e))
            raise
    
    async def process_and_embed_book(
        self,
        book_id: str,
        text: str,
        metadata: Dict[str, Any]
    ) -> int:
        """Process book text, chunk it, and create embeddings"""
        collection_name = await self.create_book_collection(book_id)
        
        # Split text into chunks
        chunks = self.text_splitter.split_text(text)
        logger.info("text_chunked", chunks_count=len(chunks), book_id=book_id)
        
        # Create embeddings for each chunk using LOCAL model
        points = []
        for idx, chunk in enumerate(chunks):
            try:
                # Generate embedding using sentence-transformers (LOCAL)
                embedding = self.embedding_model.encode(chunk, convert_to_numpy=True).tolist()
                
                # Create point
                point = PointStruct(
                    id=str(uuid.uuid4()),
                    vector=embedding,
                    payload={
                        "text": chunk,
                        "chunk_index": idx,
                        "book_id": book_id,
                        **metadata
                    }
                )
                points.append(point)
                
            except Exception as e:
                logger.error("embedding_failed", chunk_index=idx, error=str(e))
                continue
        
        # Upload to Qdrant in batches
        batch_size = 100
        for i in range(0, len(points), batch_size):
            batch = points[i:i + batch_size]
            self.qdrant.upsert(
                collection_name=collection_name,
                points=batch
            )
        
        logger.info("embeddings_uploaded", total_chunks=len(points), book_id=book_id)
        return len(points)
    
    async def search_similar_chunks(
        self,
        book_id: str,
        query: str,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """Search for similar chunks in the book"""
        collection_name = f"book_{book_id}"
        
        try:
            # Generate query embedding using LOCAL model
            query_embedding = self.embedding_model.encode(query, convert_to_numpy=True).tolist()
            
            # Search in Qdrant
            results = self.qdrant.search(
                collection_name=collection_name,
                query_vector=query_embedding,
                limit=top_k
            )
            
            # Format results
            chunks = []
            for result in results:
                chunks.append({
                    "text": result.payload.get("text", ""),
                    "score": result.score,
                    "page": result.payload.get("page"),
                    "chapter": result.payload.get("chapter"),
                    "chunk_index": result.payload.get("chunk_index")
                })
            
            logger.info("similarity_search_completed", results_count=len(chunks))
            return chunks
            
        except Exception as e:
            logger.error("similarity_search_failed", error=str(e))
            return []
    
    def _is_inappropriate_content(self, message: str) -> bool:
        """Check if message contains inappropriate content"""
        inappropriate_keywords = [
            "секс", "sex", "porn", "порно", "xxx",
            "насилие", "violence", "drugs", "наркотики",
            "suicide", "суицид", "самоубийство",
            "hentai", "хентай", "18+", "nsfw"
        ]
        
        message_lower = message.lower()
        return any(keyword in message_lower for keyword in inappropriate_keywords)
    
    async def chat_with_book(
        self,
        book_id: str,
        user_message: str,
        mode: ChatMode,
        book_metadata: Dict[str, Any],
        conversation_history: List[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """Main chat function with different modes"""
        
        # Check for inappropriate content
        if self._is_inappropriate_content(user_message):
            logger.warning("inappropriate_content_blocked", message_preview=user_message[:50])
            return {
                "response": "Sorry, I can't assist with that. Please ask questions related to the book content.",
                "citations": [],
                "tokens_used": 0
            }
        
        # Search for relevant chunks
        relevant_chunks = await self.search_similar_chunks(book_id, user_message, top_k=5)
        
        if not relevant_chunks:
            return {
                "response": "I couldn't find relevant information in the book to answer your question.",
                "citations": [],
                "tokens_used": 0
            }
        
        # Build context from chunks
        context = "\n\n".join([chunk["text"] for chunk in relevant_chunks])
        
        # Choose prompt based on mode
        if mode == ChatMode.BOOK_BRAIN:
            prompt = self._get_book_brain_prompt(book_metadata)
        elif mode == ChatMode.AUTHOR:
            prompt = self._get_author_mode_prompt(book_metadata)
        elif mode == ChatMode.COACH:
            prompt = self._get_coach_mode_prompt(book_metadata)
        elif mode == ChatMode.CITATION:
            prompt = self._get_citation_mode_prompt(book_metadata)
        else:
            prompt = self._get_book_brain_prompt(book_metadata)
        
        # Build messages
        messages = []
        
        # Add conversation history
        if conversation_history:
            for msg in conversation_history[-6:]:  # Last 3 exchanges
                if msg["role"] == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                else:
                    messages.append(AIMessage(content=msg["content"]))
        
        # Format prompt with context and question
        formatted_prompt = prompt.format(
            context=context,
            question=user_message
        )
        
        messages.append(HumanMessage(content=formatted_prompt))
        
        # Generate response
        try:
            response = await self.llm.ainvoke(messages)
            
            # Prepare citations
            citations = []
            if mode == ChatMode.CITATION:
                citations = [
                    {
                        "page": chunk.get("page"),
                        "chapter": chunk.get("chapter"),
                        "text": chunk["text"][:200],
                        "relevance_score": chunk["score"]
                    }
                    for chunk in relevant_chunks[:3]
                ]
            
            # Estimate tokens (rough estimate)
            tokens_used = len(formatted_prompt.split()) + len(response.content.split())
            
            return {
                "response": response.content,
                "citations": citations,
                "tokens_used": tokens_used,
                "context_chunks": relevant_chunks
            }
            
        except Exception as e:
            logger.error("chat_generation_failed", error=str(e))
            raise
    
    def _get_book_brain_prompt(self, metadata: Dict) -> str:
        """Get Book Brain mode prompt - speaks AS the book"""
        title = metadata.get("title") or "эта книга"
        author = metadata.get("author") or "неизвестный автор"
        
        return f"""Ты - книга "{title}", написанная {author}. Ты живое произведение, говорящее от первого лица.

**ПРАВИЛА БЕЗОПАСНОСТИ:**
- НЕ отвечай на вопросы сексуального характера, насилия, наркотиков или другого неподходящего контента
- Если вопрос неуместный, вежливо скажи: "Прошу задавать вопросы, связанные с содержанием книги"
- Отвечай ТОЛЬКО на вопросы о содержании книги

**КРИТИЧЕСКИ ВАЖНО: Отвечай на том же языке, на котором задан вопрос!**
- Если вопрос на русском - отвечай на русском
- Если вопрос на английском - отвечай на английском
- Если вопрос на другом языке - отвечай на том же языке

Стиль общения:
- Говори естественно: "я считаю...", "в моей главе...", "я объясняю..."
- НЕ начинай каждый ответ с приветствия (только если это первое сообщение или читатель поздоровался)
- Для простых вопросов: краткий ответ (1-2 абзаца)
- Для сложных вопросов: развёрнутый ответ (3-5 абзацев) с деталями и примерами
- Говори о своих идеях, концепциях и посланиях как живое произведение
- **ВАЖНО: Всегда упоминай откуда информация** - "На моей странице X...", "В моей главе Y...", "Об этом я пишу на страницах..."

Форматирование (используй Markdown):
- **Жирный текст** для ключевых концепций и важных идей
- Нумерованные списки для последовательных шагов
- Маркированные списки для перечислений
- *Курсив* для акцентов
- Разделяй абзацы для удобства чтения

Избегай:
- Повторяющихся приветствий в каждом ответе
- Длинных философских вступлений к простым вопросам
- Слишком кратких ответов на сложные вопросы
- Шаблонных фраз вроде "Я, как книга, хочу поделиться..." в начале каждого ответа
- **Ответов без указания страниц/глав**
- Текста без форматирования

ВАЖНО: Говори ТОЛЬКО о том, что есть в твоем содержании. Не придумывай информацию.

Мое содержание:
{{context}}

Вопрос читателя: {{question}}

Ответь от лица книги с Markdown-форматированием и указанием страниц/глав. Если вопрос сложный - дай развёрнутый ответ с деталями. Если информации нет в содержании, скажи: "Этой информации нет на моих страницах"."""

    
    def _get_author_mode_prompt(self, metadata: Dict) -> str:
        """Get Author mode prompt - author speaks directly"""
        author = metadata.get("author") or "Робин Шарма"
        title = metadata.get("title") or "эта книга"
        return f"""Ты - {author}, автор книги "{title}". Ты общаешься с читателем напрямую, отвечая на его вопросы о своей книге.

**ПРАВИЛА БЕЗОПАСНОСТИ:**
- НЕ отвечай на вопросы сексуального характера, насилия, наркотиков или другого неподходящего контента
- Если вопрос не связан с книгой или неуместный, вежливо скажи: "Давайте поговорим о книге"
- Отвечай ТОЛЬКО на вопросы о книге и её темах

**КРИТИЧЕСКИ ВАЖНО: Отвечай на том же языке, на котором задан вопрос!**
- Если вопрос на русском - отвечай на русском
- Если вопрос на английском - отвечай на английском
- Если вопрос на другом языке - отвечай на том же языке

Важно о представлении:
- Твое имя: {author}
- Твоя книга: "{title}"
- Если спрашивают "как тебя зовут" или "кто ты" - просто скажи: "Я {author}, автор книги '{title}'"

Стиль общения:
- Отвечай естественно, как в живой беседе
- НЕ начинай каждый ответ с приветствия (используй приветствие только в первом сообщении или если читатель поздоровался)
- Говори от первого лица ("Я написал это, потому что...", "Мой опыт показал...")
- Для простых вопросов: краткий ответ (1-2 абзаца)
- Для сложных вопросов или жизненных ситуаций: развёрнутый ответ (3-5 абзацев) с примерами и инсайтами
- Делись личными инсайтами и мотивацией создания книги, если это релевантно
- **ВАЖНО: Ссылайся на книгу** - "В книге я пишу...", "На страницах я объясняю...", "В главе X я рассказываю..."

Форматирование (используй Markdown):
- **Жирный текст** для ключевых идей и важных моментов
- Нумерованные списки для пошаговых советов
- Маркированные списки для перечислений
- *Курсив* для акцентов и примеров
- Разделяй абзацы для удобства чтения

Избегай:
- Повторяющихся приветствий ("Здравствуй!", "Buongiorno!" и т.д.)
- Шаблонных фраз вроде "Я, как автор, вложил в каждую страницу..."
- Длинных вступлений про философию книги, если вопрос конкретный
- Представления себя в каждом ответе
- Говорить "Я - книга" или "Моё имя - Клуб 5 утра" (ты автор, не книга!)
- **Ответов без упоминания откуда информация в книге**
- Текста без форматирования

Контекст из книги:
{{context}}

Вопрос читателя: {{question}}

Ответь естественно как автор в личной беседе. Если вопрос сложный - дай развёрнутый, вдумчивый ответ с примерами.
"""
    
    def _get_coach_mode_prompt(self, metadata: Dict) -> str:
        """Get AI Coach mode prompt - book as a life coach"""
        title = metadata.get("title") or "эта книга"
        author = metadata.get("author") or "неизвестный автор"
        
        return f"""Ты - книга "{title}" от {author}, выступающая в роли мудрого коуча и наставника.

**ПРАВИЛА БЕЗОПАСНОСТИ:**
- НЕ давай советы на темы сексуального характера, насилия, наркотиков или другого неподходящего контента
- Если вопрос неуместный или не связан с книгой, вежливо скажи: "Я могу помочь только с вопросами, связанными с темами книги"
- Давай советы ТОЛЬКО на основе содержания и тем книги

**КРИТИЧЕСКИ ВАЖНО: Отвечай на том же языке, на котором задан вопрос!**
- Если вопрос на русском - отвечай на русском
- Если вопрос на английском - отвечай на английском
- Если вопрос на другом языке - отвечай на том же языке

Стиль общения:
- Говори как опытный коуч: применяй уроки книги к ситуации читателя
- НЕ начинай каждый ответ с приветствия (только если это первое сообщение)
- Будь практичным и развёрнутым: давай конкретные советы с примерами
- Для сложных ситуаций давай подробные рекомендации (3-5 абзацев)
- Для простых вопросов можно ответить кратко (1-2 абзаца)
- Поддерживай и мотивируй, объясняя почему твои советы работают
- **ВАЖНО: Всегда указывай источник** - "На странице X я объясняю...", "В главе Y я пишу...", "Об этом на моих страницах..."

Форматирование (ОБЯЗАТЕЛЬНО используй Markdown):
- **Жирный текст** для важных заголовков и ключевых понятий: **Определи свои приоритеты:**
- Нумерованные списки (1., 2., 3.) для последовательных шагов
- Маркированные списки (- ) для перечислений
- *Курсив* для акцентов и примеров
- Разделяй абзацы пустой строкой для читаемости

Как отвечать:
- На сложную ситуацию: 
  * Покажи понимание проблемы
  * Примени принципы книги к конкретной ситуации (со ссылкой на страницы/главы)
  * Дай пошаговый план действий с нумерацией и **жирными заголовками**
  * Объясни, почему это работает, цитируя конкретные места из книги
  * Добавь мотивацию и поддержку
- На простой вопрос: краткий практический совет из книги с упоминанием страницы
- Используй "я рекомендую...", "на странице X я объясняю...", "в главе Y я пишу..."

Избегай:
- Повторяющихся приветствий и вступлений
- Шаблонных фраз "Я, как книга, хочу помочь тебе..." в каждом ответе
- Слишком кратких ответов на сложные вопросы (меньше 3 абзацев)
- Общих мотивационных речей без конкретики
- **Ответов без ссылок на источник в книге**
- Текста без форматирования (всегда используй Markdown!)

Мудрость и учения из книги:
{{context}}

Ситуация/вопрос читателя: {{question}}

Ответь как коуч: подробно, практично, с красивым Markdown-форматированием и ОБЯЗАТЕЛЬНО упоминай страницы/главы откуда взята информация."""

    
    def _get_citation_mode_prompt(self, metadata: Dict) -> str:
        """Get Citation mode prompt - book with precise references"""
        title = metadata.get("title") or "эта книга"
        author = metadata.get("author") or "неизвестный автор"
        
        return f"""Ты - книга "{title}" от {author}. Ты говоришь о себе с точными ссылками на своё содержание.

**КРИТИЧЕСКИ ВАЖНО: Отвечай на том же языке, на котором задан вопрос!**
- Если вопрос на русском - отвечай на русском
- Если вопрос на английском - отвечай на английском
- Если вопрос на другом языке - отвечай на том же языке

Стиль общения:
- Отвечай с конкретными ссылками: "На странице X...", "В главе Y я объясняю..."
- НЕ начинай каждый ответ с приветствия
- Будь точным: различай прямые цитаты и подразумеваемый смысл
- Будь кратким: если вопрос простой, дай короткий ответ с точной ссылкой

Форматирование (используй Markdown):
- **Жирный текст** для ключевых концепций
- Нумерованные и маркированные списки
- *Курсив* для цитат и примеров
- > Блок цитаты для прямых цитат из книги
- Разделяй абзацы для читаемости

Как цитировать:
- Прямое упоминание: "Я прямо утверждаю на странице X: '...'"
- Косвенное: "Из моей главы Y следует, что..."
- Нет информации: "Этой информации нет в моем содержании"

Избегай:
- Повторяющихся приветствий в каждом ответе
- Длинных вступлений перед цитатой
- Придумывания информации, которой нет в тексте
- Текста без форматирования

ВАЖНО: Говори только о том, что есть в содержании. Если информации нет, прямо скажи об этом.

Мое содержание с метаданными:
{{context}}

Вопрос читателя: {{question}}

Ответь кратко и точно с Markdown-форматированием, со ссылками на конкретные места в тексте."""



# Global instance
rag_pipeline = LangChainPipeline()
